

# Hands on NLP with Transformers



This training will provide an introduction to the novel transformer architecture which is currently considered state of the art for modern NLP tasks. We will take a deep dive into what makes the transformer unique in its ability to process natural language including attention and encoder-decoder architectures. We will see several examples of how people and companies are using transformers to solve a wide variety of NLP tasks including conversation-holding, image captioning, reading comprehension, and more.

This training will feature several code-driven examples of transformer-derived architectures including BERT, GPT, T5, and the Vision Transformer. Each of our case studies will be inspired by real use-cases and will lean on transfer learning to expedite our process while using actionable metrics to drive results.
### Notebooks
## Notebooks

1. [Classification with BERT vs ChatGPT](notebooks/BERT%20vs%20GPT%20for%20CLF.ipynb) - [Open in Colab](https://colab.research.google.com/drive/1elfu-6gaj0KWtIQMyeHYWqqkNUgA6hFn?usp=sharing)
2. [Classification with XLNET](notebooks/xlnet_clf.ipynb)
3. [Off the shelf NLP with T5](notebooks/t5.ipynb)
4. [Generating LaTeX with GPT2](notebooks/latex_gpt2.ipynb) - [Open in Colab](https://colab.research.google.com/drive/1bzy6QGa-IwD45LhAI0Hm6BYdORuguNS_?usp=sharing)
5. [Image Captioning with Vision Transformers](notebooks/image_captioning_vision_transformer.ipynb) - [Open in Colab](https://colab.research.google.com/drive/1OQlX_cD4mVo8vB3A4co1JIfl9Vt7rhzN?usp=sharing)
6. [3rd Party Transformer Models](notebooks/other_transformers.ipynb)

